{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torchvision import models\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import cv2\n",
    "import nltk\n",
    "from datasets import load_dataset, Image\n",
    "import requests\n",
    "import PIL.Image as pillow_image\n",
    "from nltk import tokenize\n",
    "from torchvision import transforms\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    num_epochs = 30\n",
    "    model_output_path = \"shiryoku_icm\"\n",
    "    model_filename = \"shiryoku_vision.pth\"\n",
    "    lr = 0.01\n",
    "    momentum = 0.9\n",
    "    batch_size = 64\n",
    "    top_k = 2\n",
    "    num_experts = 4\n",
    "    image_size = 224\n",
    "    train_size = 0.95\n",
    "    embed_size = 512\n",
    "    hidden_size = 1024\n",
    "    vocab_size = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For image data\n",
    "def image_transforms(image_file):\n",
    "    trasnformed_image = transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomCrop(Config.image_size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return trasnformed_image\n",
    "\n",
    "\n",
    "def read_img(image_file):\n",
    "    image = cv2.imread(image_file)\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = image.transpose(2, 0, 1)\n",
    "    return image\n",
    "\n",
    "\n",
    "def get_image_from_url(url):\n",
    "    try:\n",
    "        url_content = requests.get(url, stream=True).raw\n",
    "        image = pillow_image.open(url_content)\n",
    "        return image\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "# For text data\n",
    "def tokenize_text(text_input):\n",
    "    text_tokens = tokenize.word_tokenize(str(text_input).lower())\n",
    "\n",
    "    return text_tokens\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
    "    tokens = tokenize_text(text)\n",
    "    tokens = [t for t in tokens if t not in nltk.corpus.stopwords.words(\"english\")]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def create_vocabulary(text_dataset):\n",
    "    all_tokens = []\n",
    "    for text in text_dataset:\n",
    "        tokens = preprocess_text(text)\n",
    "        all_tokens.extend(tokens)\n",
    "\n",
    "    vocab_counter = Counter(all_tokens)\n",
    "    vocab = [word for word, count in vocab_counter.most_common() if count >= 1]\n",
    "    vocab = [\"<pad>\", \"<start>\", \"<end>\", \"<unk>\"] + vocab\n",
    "\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    idx_to_word = {idx: word for idx, word in enumerate(vocab)}\n",
    "\n",
    "    return word_to_idx, idx_to_word\n",
    "\n",
    "\n",
    "# for fetching the required datasets\n",
    "def get_dataset(data_split):\n",
    "    docci_dataset = load_dataset(\"google/docci\")\n",
    "\n",
    "    data = docci_dataset[data_split]  # type: ignore\n",
    "\n",
    "    image = data[\"image\"]\n",
    "    descriptions = data[\"description\"]\n",
    "\n",
    "    images = [Image(x) for x in image]\n",
    "    img_captions = [caption for caption in descriptions]\n",
    "\n",
    "    return images, img_captions\n",
    "\n",
    "\n",
    "def get_moondream_dataset():\n",
    "    moondream_dataset = load_dataset(\"isidentical/moondream2-coyo-5M-captions\")\n",
    "    md_data = moondream_dataset[\"train\"]  # type: ignore\n",
    "\n",
    "    image_urls = md_data[\"url\"]  # type: ignore\n",
    "    descriptions = md_data[\"moondream2_caption\"]  # type: ignore\n",
    "\n",
    "    images = [get_image_from_url(img_url) for img_url in image_urls]\n",
    "    captions = [caption for caption in descriptions]\n",
    "\n",
    "    return images, captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "\n",
    "class ConvNetEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_net = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ReLU().nn.Conv2d(32, 64, kernel_size=3),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 7 * 7, 256),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.BatchNorm1d(128, momentum=0.01),\n",
    "        )\n",
    "\n",
    "    def forward(self, image):\n",
    "        encoded_image = self.conv_net(image)\n",
    "\n",
    "        return encoded_image\n",
    "\n",
    "\n",
    "class PretrainedConvNet(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet152(pretrained=True)\n",
    "        resnet_modules = list(resnet.children())[:-1]\n",
    "\n",
    "        self.resnet = nn.Sequential(*resnet_modules)\n",
    "        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        self.batch_norm = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "\n",
    "    def forward(self, images):\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet(images)\n",
    "\n",
    "        features = features.reshape(features.size(0), -1)\n",
    "        features = self.batch_norm(self.linear(features))\n",
    "\n",
    "        return features\n",
    "\n",
    "\n",
    "class TextRNNDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, num_layers=4, max_len=20):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_size, num_layers)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def forward(self, features, captions, lengths):\n",
    "        text_embed = self.embed(captions)\n",
    "        embeddings = torch.cat((features.unsqueeze(1), text_embed), dim=2)\n",
    "        packed = pack_padded_sequence(embeddings, lengths, batch_first=True)\n",
    "        hidden, _ = self.lstm(packed)\n",
    "        rnn_output = self.linear(hidden[0])\n",
    "\n",
    "        return rnn_output\n",
    "\n",
    "    def sample(self, features, state=None):\n",
    "        sampled = []\n",
    "        input_seq = features.unsqueeze(1)\n",
    "        for _ in range(self.max_len):\n",
    "            hidden, state = self.lstm(input_seq, state)\n",
    "            outputs = self.linear(hidden.squeeze(1))\n",
    "            _, predicted = outputs.max(1)\n",
    "            sampled.append(predicted)\n",
    "            input_seq = self.embed(predicted)\n",
    "            input_seq = input_seq.unsqueeze(1)\n",
    "\n",
    "        sampled = torch.stack(sampled, 1)\n",
    "\n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "images, captions = get_moondream_dataset()\n",
    "\n",
    "captions_vocab = create_vocabulary(captions)\n",
    "\n",
    "\n",
    "class ImageCaptionData(Dataset):\n",
    "    def __init__(self, images, captions, transforms, device):\n",
    "        super().__init__()\n",
    "        self.images = images\n",
    "        self.captions = captions\n",
    "        self.transform = transforms\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        image = read_img(self.images[idx])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        image = torch.tensor(image, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        caption = tokenize_text(self.captions[idx])\n",
    "\n",
    "        caption = torch.tensor(caption).to(self.device)\n",
    "\n",
    "        return image, caption\n",
    "\n",
    "\n",
    "dataset = ImageCaptionData(\n",
    "    images=images, captions=captions_vocab, transforms=image_transforms, device=device\n",
    ")\n",
    "\n",
    "train_size = 0.95 * len(dataset)\n",
    "val_size = len(dataset) - train_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageCaptionData(\n",
    "    images=images, captions=captions_vocab, transforms=image_transforms, device=device\n",
    ")\n",
    "\n",
    "train_size = 0.95 * len(dataset)\n",
    "\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_data, valid_data = random_split(dataset, (train_size, val_size))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=Config.batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=Config.batch_size, shuffle=False)\n",
    "\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = PretrainedConvNet(embed_size=Config.embed_size)\n",
    "decoder = TextRNNDecoder(\n",
    "    vocab_size=Config.vocab_size,\n",
    "    embed_dim=Config.embed_size,\n",
    "    hidden_size=Config.hidden_size,\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "parameters = (\n",
    "    list(decoder.parameters())\n",
    "    + list(encoder.linear.parameters())\n",
    "    + list(encoder.batch_norm.parameters())\n",
    ")\n",
    "optimizer = optim.Adam(params=parameters, lr=Config.lr)\n",
    "epochs = Config.num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(train_loader, lossfn, optimizer, epochs=epochs):\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        print(f\"Training epoch {epoch}\")\n",
    "        for _, (images, captions, lengths) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
    "\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions, lengths)\n",
    "            loss = lossfn(outputs, targets)\n",
    "\n",
    "            decoder.zero_grad()\n",
    "            encoder.zero_grad()\n",
    "            loss.backwards()\n",
    "            optimizer.step()\n",
    "\n",
    "            print(f\"Epoch {epoch} of {epochs}, loss: {loss.item():.4f}\")\n",
    "\n",
    "        print(f\"End metrics for {epoch} of {epochs}, loss: {loss.item():.4f}\")\n",
    "\n",
    "        torch.save(\n",
    "            decoder.state_dict(),\n",
    "            os.path.join(Config.model_output_path, f\"decoder_{epoch}.pth\"),\n",
    "        )\n",
    "\n",
    "        torch.save(\n",
    "            encoder.state_dict(),\n",
    "            os.path.join(Config.model_output_path, f\"encoder_{epoch}.pth\"),\n",
    "        )\n",
    "\n",
    "        print(f\"Epoch {epoch} complete\")\n",
    "\n",
    "    print(f'Training complete')\n",
    "    torch.save(\n",
    "            decoder.state_dict(),\n",
    "            os.path.join(Config.model_output_path, f\"decoder_{Config.model_filename}\"),\n",
    "        )\n",
    "\n",
    "    torch.save(\n",
    "            encoder.state_dict(),\n",
    "            os.path.join(Config.model_output_path, f\"encoder_{Config.model_filename}\"),\n",
    "        )\n",
    "    print('Models saved')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".vit_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
